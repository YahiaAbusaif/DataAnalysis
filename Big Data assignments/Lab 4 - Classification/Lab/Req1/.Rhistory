sum((results == '50-80K') == (testdata$income == '10-50K'))
sum((results == '10-50K') == (testdata$income == '10-50K'))
testdata$income == '10-50K'
sum(testdata$income == '10-50K')
sum((results == '10-50K') == (testdata$income == '10-50K'))
sum((results == '50-80K') == (testdata$income == '10-50K'))
v1 = c('No', 'No', 'No', 'No', 'yes', 'yes', 'No', 'yes')
v2 = c('yes', 'No', 'yes', 'No', 'yes', 'yes', 'No', 'yes')
sum((v1=='No') == (v2=='No'))
sum(v1 == v2)
v1
v2
v1 == v2
t1 = v1 == 'No'
t2 = v2 == 'yes'
sum(t1 == t2)
t1 = v1 == 'yes'
t2 = v2 == 'no'
sum(t1 == t2)
sum(t1 == t2 && t1 == TRUE)
No
No
t1 = v1 == 'No'
t2 = v2 == 'yes'
sum(t1 == t2 && t1 == TRUE)
sum(t1 == t2 && (t1 == TRUE))
sum(t1 == t2 & (t1 == TRUE))
sum(t1 == t2 & t1 == TRUE)
sum((v1=='yes') == (v2=='No'))
v1 = c('No', 'yes', 'No', 'No', 'yes', 'yes', 'No', 'yes')
v2 = c('yes', 'No', 'yes', 'No', 'yes', 'yes', 'No', 'yes')
t1 = v1 == 'No'
t2 = v2 == 'yes'
sum(t1 == t2 && (t1 == TRUE))
t1 = v1 == 'no'
t2 = v2 == 'yes'
sum(t1 == t2 && (t1 == TRUE))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:9110,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
v1 = c('No', 'yes', 'No', 'No', 'yes', 'yes', 'No', 'yes')
v2 = c('yes', 'No', 'yes', 'No', 'yes', 'yes', 'No', 'yes')
t1 = v1 == 'no'
t2 = v2 == 'yes'
t1 = v1 == 'No'
sum(t1 == t2 && (t1 == TRUE))
sum(t1 == t2 & (t1 == TRUE))
v1 = c('No', 'yes', 'No', 'No', 'yes', 'yes', 'No', 'yes')
v2 = c('yes', 'No', 'yes', 'yes', 'yes', 'yes', 'No', 'yes')
t1 = v1 == 'No'
t2 = v2 == 'yes'
sum(t1 == t2 & (t1 == TRUE))
t1 = v1 == 'yes'
t2 = v2 == 'No'
sum(t1 == t2 & (t1 == TRUE))
t1 = v1 == 'No'
t2 = v2 == 'No'
sum(t1 == t2 & (t1 == TRUE))
t1 = v1 == 'yes'
t2 = v2 == 'yes'
sum(t1 == t2 & (t1 == TRUE))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:9110,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
v1 = c('No', 'yes', 'No', 'No', 'yes', 'yes', 'No', 'yes')
v2 = c('yes', 'No', 'yes', 'yes', 'yes', 'yes', 'No', 'yes')
t1 = v1 == 'yes'
t2 = v2 == 'yes'
class = c('10-50K', '50-80K', 'GT 80K')
cols = 0
for (i in 1:3){
col = 0
for (j in 1:3){
v1 = testdata$income == class[i]
v2 = results == class[j]
col[j] = sum(v1 == v2 & (v1 == TRUE))
}
cols[i] = col
}
mat - matrix(nrow=3, ncol=3)
mat - matrix(data=NA, nrow=3, ncol=3)
mat = matrix(nrow=3, ncol=3)
mat
MatrixC <- cbind(mat, c(10, 11, 12))
MatrixC
mat[,1] = c(1,2,3)
mat
rnames <- c("10-50K", "50-80K","GT 80K")
cnames <- c("10-50K", "50-80K","GT 80K")
mymatrix <- matrix(cells, nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(rnames, cnames))
mymatrix <- matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(rnames, cnames))
mymatrix
mymatrix[,1] = c(1,2,3)
mymatrix
mymatrix[1,1]
mymatrix[1,2]
mymatrix[2,1]
class = c('10-50K', '50-80K', 'GT 80K')/
class = c('1', '2', '3')
rnames <- c("10-50K", "50-80K","GT 80K")
cnames <- c("10-50K", "50-80K","GT 80K")
mymatrix <- matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
mymatrix
mymatrix
class = c('10-50K', '50-80K', 'GT 80K')/
class = c('1', '2', '3')
class = c('10-50K', '50-80K', 'GT 80K')
class = c('1', '2', '3')
rnames <- c("10-50K", "50-80K","GT 80K")
cnames <- c("10-50K", "50-80K","GT 80K")
mymatrix <- matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
mymatrix
for (i in 1:3){
col = 0
for (j in 1:3){
v1 = testdata$income == class[i]
v2 = results == class[j]
col[j] = sum(v1 == v2 & (v1 == TRUE))
}
}
class = c('10-50K', '50-80K', 'GT 80K')
class = c('1', '2', '3')
class = c('10-50K', '50-80K', 'GT 80K')
class = c('1', '2', '3')
mymatrix <- matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
mymatrix
for (i in 1:3){
col = 0
for (j in 1:3){
t1 = v1 == class[i]
t2 = v2 == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
mymatrix[,i] = col
}
mymatrix
rm(list=ls())
v1 = c('1', '2', '3', '2', '1', '1', '1', '2')
v2 = c('1', '2', '1', '1', '2', '1', '1', '3')
t1 = v1 == 'yes'
t2 = v2 == 'yes'
class = c('10-50K', '50-80K', 'GT 80K')
class = c('1', '2', '3')
mymatrix <- matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
mymatrix
for (i in 1:3){
col = 0
for (j in 1:3){
t1 = v1 == class[i]
t2 = v2 == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
mymatrix[,i] = col
}
mymatrix
Confusion = matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
v1 = c('1', '2', '3', '2', '1', '1', '1', '2')
v2 = c('1', '2', '1', '1', '2', '1', '1', '3')
t1 = v1 == 'yes'
t2 = v2 == 'yes'
class = c('10-50K', '50-80K', 'GT 80K')
class = c('1', '2', '3')
Confusion = matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
Confusion
for (i in 1:3){
col = 0
t1 = v1 == class[i]
for (j in 1:3){
t2 = v2 == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
Confusion[,i] = col
}
Confusion
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:10010,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
class = c('10-50K', '50-80K', 'GT 80K')
Confusion = matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames = list(class, class))
Confusion
for (i in 1:3){
col = 0
t1 = v1 == class[i]
for (j in 1:3){
t2 = v2 == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
Confusion[,i] = col
}
class = c('10-50K', '50-80K', 'GT 80K')
Confusion = matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames=list(class, class))
Confusion
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
Confusion[,i] = col
}
Confusion
sum(results == testdata$income)
sum(results == testdata$income) / 1010
Confusion
cbind(Confusion, total = rowSums(data))
cbind(Confusion, total = rowSums(Confusion))
Confusion
cbind(Confusion, total = rowSums(Confusion))
rbind(Confusion, total = colSums(Confusion))
Confusion = cbind(Confusion, total = rowSums(Confusion))
Confusion
Confusion = cbind(Confusion, total = rowSums(Confusion))
Confusion = rbind(Confusion, total = colSums(Confusion))
Confusion
class = c('10-50K', '50-80K', 'GT 80K')
Confusion = matrix(nrow = 3, ncol = 3, byrow =TRUE, dimnames=list(class, class))
Confusion
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
Confusion[,i] = col
}
Confusion = cbind(Confusion, total = rowSums(Confusion))
Confusion = rbind(Confusion, total = colSums(Confusion))
Confusion
results == '50-80K'
sum(results == '50-80K')
sum(results == '10-50K')
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:10010,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
class = c('10-50K', '50-80K', 'GT 80K')
confusion = matrix(nrow = 3, ncol = 3, byrow=TRUE, dimnames=list(class, class))
confusion
# rows -> prediction class
# cols -> true class
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
confusion[,i] = col
}
confusion = cbind(confusion, total=rowSums(confusion))
confusion = rbind(confusion, total=colSums(confusion))
confusion
class = c('10-50K', '50-80K', 'GT 80K')
rclass = c('10-50K(P)', '50-80K(P)', 'GT 80K(P)')
cclass = c('10-50K(T)', '50-80K(T)', 'GT 80K(T)')
confusion = matrix(nrow = 3, ncol = 3, byrow=TRUE, dimnames=list(rclass, cclass))
confusion
# rows -> prediction class
# cols -> true class
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
confusion[,i] = col
}
confusion = cbind(confusion, total=rowSums(confusion))
confusion = rbind(confusion, total=colSums(confusion))
confusion
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:10010,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
#(7) Confusion Matrix
class = c('10-50K', '50-80K', 'GT 80K')
rclass = c('10-50K(P)', '50-80K(P)', 'GT 80K(P)')
cclass = c('10-50K(T)', '50-80K(T)', 'GT 80K(T)')
confusion = matrix(nrow = 3, ncol = 3, byrow=TRUE, dimnames=list(rclass, cclass))
# rows -> prediction class (P)
# cols -> true class (T)
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
confusion[,i] = col
}
confusion = cbind(confusion, total=rowSums(confusion))
confusion = rbind(confusion, total=colSums(confusion))
confusion
traindata$income == '10-50K'
sum(traindata$income == '10-50K')
sum(traindata$income == '50-80K')
sum(traindata$income == 'GT 80K')
acc = sum(results == testdata$income)
acc
acc = sum(results == testdata$income) / 1010
acc
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:10010,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
#(7) Confusion Matrix
class = c('10-50K', '50-80K', 'GT 80K')
rclass = c('10-50K(P)', '50-80K(P)', 'GT 80K(P)')
cclass = c('10-50K(T)', '50-80K(T)', 'GT 80K(T)')
confusion = matrix(nrow = 3, ncol = 3, byrow=TRUE, dimnames=list(rclass, cclass))
# rows -> prediction class (P)
# cols -> true class (T)
for (i in 1:3){
col = 0
t1 = testdata$income == class[i]
for (j in 1:3){
t2 = results == class[j]
col[j] = sum(t1 == t2 & (t1 == TRUE))
}
confusion[,i] = col
}
confusion = cbind(confusion, total=rowSums(confusion))
confusion = rbind(confusion, total=colSums(confusion))
confusion
acc = sum(results == testdata$income) / 1010
acc
## Accuracy is almost 80% which isn't bad BUT again a big portion of the test set
## Accuracy is almost 80% which isn't bad BUT again a big portion of the test set
# belongs to the class '10-50K' whice i think the model is biased to. Thus
confusion[4,1]
confusion[2,2]
confusion[3,3]
confusion[4,3]
confusion[4,2]
#(9) Missclassification rate = error rate = 1 - accuracy
## for class '10-50K'
1 - confusion[1,1]/confusion[4,1]
## for class '50-80K'
1 - confusion[2,2]/confusion[4,2]
## for class 'GT 80K'
1 - confusion[3,3]/confusion[4,3]
#(9) Misclassification rate = 1 - TPR
## for class '10-50K'
1 - confusion[1,1]/confusion[4,1]
## for class '50-80K'
1 - confusion[2,2]/confusion[4,2]
## for class 'GT 80K'
1 - confusion[3,3]/confusion[4,3]
