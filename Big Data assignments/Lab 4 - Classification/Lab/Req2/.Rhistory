association.rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', header=FALSE, sep=',')
association.rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5))
association.rules = apriori(tr, parameter = list(supp = 0.00, conf = 0.5))
association.rules = apriori(tr, parameter = list(supp = 0.00, conf = 0))
association.rules = apriori(tr, parameter = list(supp = 0.00, conf = 0, minlen=2))
association.rules = apriori(tr, parameter = list(supp = 0.00, conf = 0))
association.rules = apriori(tr, parameter = list(supp = 0.00, conf = 0))
summary(association.rules)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format = 'basket', sep=',')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)])
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format = 'basket', sep=' ')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)])
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format = 'basket', sep=',')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)])
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format = 'basket', sep=' ')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)])
summary(tr)
#(5) Frequency
freq = summary(tr)@itemSummary
# The 1st most frequent item and its frequency
freq[1]
# The 2nd most frequent item and its frequency
freq[2]
#(6) Plot the top 5 items frequencies
freq[1:5]
itemFrequencyPlot(tr, type="absolute", topN=5, main="Absolute Frequency")
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)], linebreak = FALSE)
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)], linebreak=FALSE)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', sep=' ')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)], linebreak=FALSE)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='single', sep=' ')
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='basket', sep=' ')
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='basket', sep=' ')
#(4)Display first 100
inspect(tr[1:min(length(tr), 100)], linebreak=FALSE)
#(5) Frequency
freq = summary(tr)@itemSummary
# The 1st most frequent item and its frequency
freq[1]
# The 2nd most frequent item and its frequency
freq[2]
#(6) Plot the top 5 items frequencies
freq[1:5]
itemFrequencyPlot(tr, type="absolute", topN=5, main="Absolute Frequency")
rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5, minlen=2))
summary(rules)
inspect(rules[1:6])
head(rules)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='basket', sep=' ')
#(7) Rules by Apriori algorithm
rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5, minlen=2))
summary(rules)
sort(rules, by='support')
sort(rules, by='confidence')
sort(rules, by='lift')
sort(rules, by='support')
inspect(rules[1:6])
sort(rules, by='confidence')
inspect(rules[1:6])
head(rules, n=5)
head(rules, n=4)
rules_by_sup = sort(rules, by='support')
inspect(rules_by_sup[1:6])
rules_by_conf = sort(rules, by='confidence')
inspect(rules_by_conf[1:6])
rules_by_lift = sort(rules, by='lift')
inspect(rules_by_lift[1:6])
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='basket', sep=' ')
#(4)Display first 100
inspect(head(tr, 100), linebreak=FALSE)
#(7) Rules by Apriori algorithm
rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5, minlen=2))
summary(rules)
n = 6
rules_by_sup = sort(rules, by='support')
inspect(head(rules_by_sup, n))
rules_by_conf = sort(rules, by='confidence')
inspect(head(rules_by_conf, n))
rules_by_lift = sort(rules, by='lift')
inspect(head(rules_by_lift, n))
n = 6
rules_by_sup = sort(rules, by='support')
inspect(head(rules_by_sup, n))
rules_by_conf = sort(rules, by='confidence')
inspect(head(rules_by_conf, n))
rules_by_lift = sort(rules, by='lift')
inspect(head(rules_by_lift, n))
plot(rules)
plot(rules, measure='support', shading='lift')
plot(rules, measure='support', shading='lift')
plot(rules, measure='support', shading='lift')
plot(rules, measure='confidence', shading='lift')
plot(rules, measure='support', shading='lift')
inspect(head(rules_by_lift, n))
#(9)
rules_by_conf = sort(rules, by='confidence')
inspect(head(rules_by_conf, n))
inspect(head(rules_by_lift, n))
inspect(head(rules_by_sup, n))
inspect(head(rules_by_conf, n))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining\\Req")
#(2) Load libraries
library(arules)
library(arulesViz)
#(3)The file contains no header line
tr = read.transactions('AssociationRules.csv', format='basket', sep=' ')
#(4)Display first 100
inspect(head(tr, 100), linebreak=FALSE)
#(5) Frequency
freq = summary(tr)@itemSummary
# The 1st most frequent item and its frequency
freq[1]
# The 2nd most frequent item and its frequency
freq[2]
#(6) Plot the top 5 items frequencies
freq[1:5]
itemFrequencyPlot(tr, type="absolute", topN=5, main="Absolute Frequency")
#(7) Rules by Apriori algorithm
rules = apriori(tr, parameter = list(supp = 0.01, conf = 0.5, minlen=2))
summary(rules)
n = 6
#(8)
rules_by_sup = sort(rules, by='support')
inspect(head(rules_by_sup, n))
#(9)
rules_by_conf = sort(rules, by='confidence')
inspect(head(rules_by_conf, n))
#(10)
rules_by_lift = sort(rules, by='lift')
inspect(head(rules_by_lift, n))
#(11)
plot(rules, measure='support', shading='lift')
## The most interesting rules are those with high lift since high lift (say >> 1)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req1")
library("e1071")
sample = read.table("nbtrain.csv", header=TRUE, sep=",")
#(3) we will now define the data frames to use the NB classifier
traindata = as.data.frame(sample[1:9000,])
testdata = as.data.frame(sample[9001:10010,])
## Comment
# We split the data to train, test to evaluate the model on unseen examples (test set).
# Since the model is expected to perform well on seen data (train), we need to see how
# general the model is by testing it on unseen data (test).
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
# Step 2: ... Conditional Probabilities
ageCounts <-table(traindata[,c("income", "age")])
# Laplace smoothing is a way to solve the problem of "Zero Probability" by adding
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
ageCounts <-table(traindata[,c("income", "age")])
ageCounts
ageCounts = ageCounts/rowSums(ageCounts)
ageCounts
#(4)
model = naiveBayes(income~., data=traindata, laplace=0)
# Laplace smoothing is a way to solve the problem of "Zero Probability" by adding
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
ageCounts = table(traindata[,c("income", "age")])
ageCounts = ageCounts/rowSums(ageCounts)
ageCounts
#(5)display model
model
ageCounts
#(5)display model
model
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# Laplace smoothing is a way to solve the problem of "Zero Probability" by adding
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
ageCounts = table(traindata[,c("income", "age")])
ageCounts = ageCounts/rowSums(ageCounts)
ageCounts
#(5)display model
model
#(6)
results = predict (model, testdata)
#(4)
model = naiveBayes(income~., data=traindata, laplace=0.01)
# Laplace smoothing is a way to solve the problem of "Zero Probability" by adding
# like fake samples, for example if X = (.., age=20-30,..)
# and P(age=20-30|y=10-50K) = 0 this will yield
# Conditional prob. to be zero P(X|y=10-50K) = 0.
# To avoid that we use the following rule P(X=xi|Y=yi) = (x(i) + l)/(N + l*d)
# N -> number of samples of class yi.
# x(i) -> number of samples of feature x = i within the class yi.
# l -> laplace coeff.
# d -> number of categories x can take.
### EXAMPLE
ageCounts = table(traindata[,c("income", "age")])
ageCounts
ageCounts = ageCounts/rowSums(ageCounts)
ageCounts
#(5)display model
model
#(6)
results = predict (model, testdata)
# display results
results
#(7) Confusion Matrix
confusion = table(results, testdata$income, dnn=list('Predict', 'Actual'))
confusion = cbind(confusion, total=rowSums(confusion))
confusion = rbind(confusion, total=colSums(confusion))
# cols -> Actual class (T)
# rows -> Predict class (P)
confusion
acc = sum(results == testdata$income) / 1010
acc
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req2")
#install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
library("ROCR")
#Read the data
play_decision <- read.table("DTdata.csv",header=TRUE,sep=",")
play_decision
summary(play_decision)
#Build the tree to "fit" the model
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### minsplit -> the min number of observations in a node so a split is attempted.
### Ex: minsplit = 7 --> there will be non-terminal nodes with less than 7 obsrvations.
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=7, maxdepth = 3, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### maxdepth -> the max depth of the tree.
### Ex: maxdepth = 1 --> will only have root (depth 0) and terminal nodes (depth 1) only
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 1, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### minbucket --> the min number of observations in a terminal node
### minbucket = 2, terminal nodes with observations less than 2 will be deleted.
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3, minbucket = 2),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
#Q5: Plot the tree with propabilities instead of number of observations in each node.
##A5: extra = 4
rpart.plot(fit, type = 4, extra = 4)
#Predict if Play is possible for condition rainy, mild humidity, high temperature and no wind
newdata <- data.frame(Outlook="overcast",Temperature="mild",Humidity="high",Wind=FALSE)
newdata
predict(fit,newdata=newdata,type=c("class"))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req2")
#install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
library("ROCR")
#Read the data
play_decision <- read.table("DTdata.csv",header=TRUE,sep=",")
play_decision
summary(play_decision)
#Build the tree to "fit" the model
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### minsplit -> the min number of observations in a node so a split is attempted.
### Ex: minsplit = 7 --> there will be non-terminal nodes with less than 7 obsrvations.
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=7, maxdepth = 3, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### maxdepth -> the max depth of the tree.
### Ex: maxdepth = 1 --> will only have root (depth 0) and terminal nodes (depth 1) only
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 1, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### minbucket --> the min number of observations in a terminal node
### minbucket = 2, terminal nodes with observations less than 2 will be deleted.
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3, minbucket = 2),
parms=list(split='information'))
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
#Q5: Plot the tree with propabilities instead of number of observations in each node.
##A5: extra = 4
rpart.plot(fit, type = 4, extra = 4)
#Predict if Play is possible for condition rainy, mild humidity, high temperature and no wind
newdata <- data.frame(Outlook="overcast",Temperature="mild",Humidity="high",Wind=FALSE)
newdata
predict(fit,newdata=newdata,type=c("class"))
######################################################################################
######################################################################################
#Q6: What is the predicted class for this test case?
######################################################################################
#Q6: What is the predicted class for this test case?
#A6: The predicted class is "yes"
######################################################################################
#Q6: What is the predicted class for this test case?
#A6: The predicted class is "yes"
#Q7: State the sequence of tree node checks to reach this class (label).
######################################################################################
#Q6: What is the predicted class for this test case?
#A6: The predicted class is "yes"
#Q7: State the sequence of tree node checks to reach this class (label).
#A7: The sequence = node 1 ->(Temperature="mild)-> node 2 ->Outlook="overcast"-> node 5 (leaf)
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req2")
#install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
library("ROCR")
#Read the data
play_decision <- read.table("DTdata.csv",header=TRUE,sep=",")
play_decision
summary(play_decision)
#Build the tree to "fit" the model
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
#Q5: Plot the tree with propabilities instead of number of observations in each node.
##A5: extra = 4
rpart.plot(fit, type = 4, extra = 4)
#Predict if Play is possible for condition rainy, mild humidity, high temperature and no wind
newdata <- data.frame(Outlook="overcast",Temperature="mild",Humidity="high",Wind=FALSE)
newdata
predict(fit,newdata=newdata,type=c("class"))
rm(list=ls())
setwd("F:\\CMP 2020\\Big Data\\Labs\\Lab 4 - Classification\\Lab 4 - Classification\\Lab\\Req2")
#install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
library("ROCR")
#Read the data
play_decision <- read.table("DTdata.csv",header=TRUE,sep=",")
play_decision
summary(play_decision)
#Build the tree to "fit" the model
fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit, type = 4, extra = 1)
summary(fit)
### minsplit -> the min number of observations in a node so a split is attempted.
### Ex: minsplit = 7 --> there will be non-terminal nodes with less than 7 obsrvations.
fit1 <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=7, maxdepth = 3, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit1, type = 4, extra = 1)
summary(fit1)
### maxdepth -> the max depth of the tree.
### Ex: maxdepth = 1 --> will only have root (depth 0) and terminal nodes (depth 1) only
fit2 <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 1, minbucket = 1),
parms=list(split='information'))
rpart.plot(fit2, type = 4, extra = 1)
summary(fit2)
### minbucket --> the min number of observations in a terminal node
### minbucket = 2, terminal nodes with observations less than 2 will be deleted.
fit3 <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3, minbucket = 2),
parms=list(split='information'))
rpart.plot(fit3, type = 4, extra = 1)
summary(fit3)
#Q5: Plot the tree with propabilities instead of number of observations in each node.
##A5: extra = 4
rpart.plot(fit, type = 4, extra = 4)
#Predict if Play is possible for condition rainy, mild humidity, high temperature and no wind
newdata <- data.frame(Outlook="overcast",Temperature="mild",Humidity="high",Wind=FALSE)
newdata
predict(fit,newdata=newdata,type=c("class"))
